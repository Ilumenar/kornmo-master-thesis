{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sentinel.storage import SentinelDataset\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tqdm import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "import geopandas as gpd\n",
    "\n",
    "\n",
    "CLASSES = ['bygg', 'rug', 'rughvete', 'hvete', 'havre']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "labels_lookup = gpd.read_file('../../kornmo-data-files/raw-data/crop-classification-data/all_data.gpkg')\n",
    "labels_lookup[\"key\"] = labels_lookup.orgnr.astype(int).astype(str) + labels_lookup.index.astype(str) + \"/\" + labels_lookup.year.astype(int).astype(str)\n",
    "labels_lookup.drop(columns=[\"year\", \"orgnr\"], inplace=True)\n",
    "\n",
    "labels_lookup.drop(labels_lookup[labels_lookup['area'] < 1500].index, inplace = True)\n",
    "labels_lookup = labels_lookup.loc[labels_lookup['planted'] != 'erter']\n",
    "labels_lookup = labels_lookup.loc[labels_lookup['planted'] != 'oljefro']\n",
    "\n",
    "labels_lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "labels_to_keep = [f\"{int(label.split('/')[1])}/{int(label.split('/')[2])}\" for label in sd.labels]\n",
    "\n",
    "print(pd.Series(list(labels_lookup['planted'])).value_counts())\n",
    "labels_lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sd = SentinelDataset('E:/MasterThesisData/Satellite_Images/small_images_all.h5')\n",
    "all_images = sd.to_iterator().shuffled()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "unique = labels_lookup['key'].unique()\n",
    "\n",
    "data_x = []\n",
    "data_y = []\n",
    "max_imgs = 15000\n",
    "i = 0\n",
    "\n",
    "# Loads 15000 images including their labels directly into memory\n",
    "for data in tqdm(all_images, total=max_imgs):\n",
    "\n",
    "    data_x.append(data[2][()])\n",
    "    if f\"{int(data[0])}/{int(data[1])}\" in unique:\n",
    "        label = labels_lookup.loc[labels_lookup['key'] == f\"{int(data[0])}/{int(data[1])}\"]['planted'].iloc[0]\n",
    "        data_y.append(label)\n",
    "    else:\n",
    "        data_y.append(None)\n",
    "    if i >= max_imgs - 1:\n",
    "        break\n",
    "    i += 1\n",
    "\n",
    "\n",
    "#print(np.array(data_x).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(data_x, data_y, test_size=0.3, random_state=42)\n",
    "del data_x, data_y\n",
    "print(f'x_train: {len(x_train)}')\n",
    "print(f'x_val: {len(x_val)}')\n",
    "print(f'y_train: {len(y_train)}')\n",
    "print(f'y_val: {len(y_val)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(x_train[0])\n",
    "print(y_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "BAG_COUNT = 100\n",
    "VAL_BAG_COUNT = 100\n",
    "BAG_SIZE = 7\n",
    "PLOT_SIZE = 3\n",
    "\n",
    "(BAG_COUNT + VAL_BAG_COUNT) * BAG_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def create_bags(input_data, input_labels, positive_class, bag_count, instance_count):\n",
    "    bags = []\n",
    "    bag_labels = []\n",
    "\n",
    "    # input_data = np.divide(input_data, 255.0)\n",
    "\n",
    "    count = 0\n",
    "    for _ in tqdm(range(bag_count), desc=f'Creating bag for {positive_class}'):\n",
    "        index = np.random.choice(input_data.shape[0], instance_count, replace=False)\n",
    "        instances_data = input_data[index]\n",
    "        instances_labels = input_labels[index]\n",
    "\n",
    "        bag_label = 0\n",
    "\n",
    "        if positive_class in instances_labels:\n",
    "            bag_label = 1\n",
    "            count += 1\n",
    "\n",
    "        bags.append(instances_data)\n",
    "        bag_labels.append(np.array([bag_label]))\n",
    "\n",
    "    print(f\"Positive bags: {count}\")\n",
    "    print(f\"Negative bags: {bag_count - count}\")\n",
    "    return list(np.swapaxes(bags, 0, 1)), np.array(bag_labels)\n",
    "\n",
    "\n",
    "train_data, train_labels = create_bags(np.array(x_train), np.array(y_train), 'rug', BAG_COUNT, BAG_SIZE)\n",
    "val_data, val_labels = create_bags(np.array(x_val), np.array(y_val), 'rug', VAL_BAG_COUNT, BAG_SIZE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class MILAttentionLayer(layers.Layer):\n",
    "\n",
    "    def  __init__(\n",
    "        self,\n",
    "        weight_params_dim,\n",
    "        kernel_initializer=\"glorot_uniform\",\n",
    "        kernel_regularizer=None,\n",
    "        use_gated=False,\n",
    "        **kwargs\n",
    "    ):\n",
    "\n",
    "        super(MILAttentionLayer, self).__init__(**kwargs)\n",
    "\n",
    "        self.weight_params_dim = weight_params_dim\n",
    "        self.use_gated = use_gated\n",
    "\n",
    "        self.kernel_initializer = keras.initializers.get(kernel_initializer)\n",
    "        self.kernel_regularizer = keras.regularizers.get(kernel_regularizer)\n",
    "\n",
    "        self.v_init = self.kernel_initializer\n",
    "        self.w_init = self.kernel_initializer\n",
    "        self.u_init = self.kernel_initializer\n",
    "\n",
    "        self.v_regularizer = self.kernel_regularizer\n",
    "        self.w_regularizer = self.kernel_regularizer\n",
    "        self.u_regularizer = self.kernel_regularizer\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(MILAttentionLayer, self).get_config()\n",
    "        config.update({\"weight_params_dim\": self.weight_params_dim})\n",
    "        config.update({\"use_gated\": self.use_gated})\n",
    "        config.update({\"kernel_initializer\": self.kernel_initializer})\n",
    "        config.update({\"kernel_regularizer\": self.kernel_regularizer})\n",
    "\n",
    "        return config\n",
    "\n",
    "    def build(self, input_shape):\n",
    "\n",
    "        # Input shape.\n",
    "        # List of 2D tensors with shape: (batch_size, input_dim).\n",
    "        input_dim = 48\n",
    "\n",
    "        self.v_weight_params = self.add_weight(\n",
    "            shape=(input_dim, self.weight_params_dim),\n",
    "            initializer=self.v_init,\n",
    "            name=\"v\",\n",
    "            regularizer=self.v_regularizer,\n",
    "            trainable=True,\n",
    "        )\n",
    "\n",
    "        self.w_weight_params = self.add_weight(\n",
    "            shape=(self.weight_params_dim, 1),\n",
    "            initializer=self.w_init,\n",
    "            name=\"w\",\n",
    "            regularizer=self.w_regularizer,\n",
    "            trainable=True,\n",
    "        )\n",
    "\n",
    "        if self.use_gated:\n",
    "            self.u_weight_params = self.add_weight(\n",
    "                shape=(input_dim, self.weight_params_dim),\n",
    "                initializer=self.u_init,\n",
    "                name=\"u\",\n",
    "                regularizer=self.u_regularizer,\n",
    "                trainable=True,\n",
    "            )\n",
    "        else:\n",
    "            self.u_weight_params = None\n",
    "\n",
    "        self.input_built = True\n",
    "\n",
    "    def call(self, inputs):\n",
    "\n",
    "        # Assigning variables from the number of inputs.\n",
    "        instances = [self.compute_attention_scores(instance) for instance in inputs]\n",
    "\n",
    "        # Apply softmax over instances such that the output summation is equal to 1.\n",
    "        alpha = tf.math.softmax(instances, axis=0)\n",
    "\n",
    "        return [alpha[i] for i in range(alpha.shape[0])]\n",
    "\n",
    "    def compute_attention_scores(self, instance):\n",
    "\n",
    "        # Reserve in-case \"gated mechanism\" used.\n",
    "        original_instance = instance\n",
    "\n",
    "        # tanh(v*h_k^T)\n",
    "        instance = tf.math.tanh(tf.tensordot(instance, self.v_weight_params, axes=1))\n",
    "\n",
    "        # for learning non-linear relations efficiently.\n",
    "        if self.use_gated:\n",
    "\n",
    "            instance = instance * tf.math.sigmoid(\n",
    "                tf.tensordot(original_instance, self.u_weight_params, axes=1)\n",
    "            )\n",
    "\n",
    "        # w^T*(tanh(v*h_k^T)) / w^T*(tanh(v*h_k^T)*sigmoid(u*h_k^T))\n",
    "        return tf.tensordot(instance, self.w_weight_params, axes=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def create_model(instance_shape):\n",
    "\n",
    "    # Extract features from inputs.\n",
    "    inputs, embeddings = [], []\n",
    "    shared_dense_layer_1 = layers.Dense(96, activation=\"relu\")\n",
    "    dropout_layer = layers.Dropout(0.6)\n",
    "    shared_dense_layer_2 = layers.Dense(48, activation=\"relu\")\n",
    "    for _ in range(BAG_SIZE):\n",
    "        inp = layers.Input(instance_shape)\n",
    "        flatten = layers.Flatten()(inp)\n",
    "        dense_1 = shared_dense_layer_1(flatten)\n",
    "        dropout = dropout_layer(dense_1)\n",
    "        dense_2 = shared_dense_layer_2(dropout)\n",
    "        inputs.append(inp)\n",
    "        embeddings.append(dense_2)\n",
    "\n",
    "    # Invoke the attention layer.\n",
    "    alpha = MILAttentionLayer(weight_params_dim=256, kernel_regularizer=keras.regularizers.l2(0.001), use_gated=True, name=\"alpha\")\n",
    "    alpha = alpha(embeddings)\n",
    "\n",
    "    # Multiply attention weights with the input layers.\n",
    "    multiply_layers = [\n",
    "        layers.multiply([alpha[i], embeddings[i]]) for i in range(len(alpha))\n",
    "    ]\n",
    "\n",
    "    # Concatenate layers.\n",
    "    concat = layers.concatenate(multiply_layers, axis=1)\n",
    "\n",
    "    # Classification output node.\n",
    "    output = layers.Dense(2, activation=\"softmax\")(concat)\n",
    "\n",
    "    return keras.Model(inputs, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def compute_class_weights(labels):\n",
    "\n",
    "    # Count number of postive and negative bags.\n",
    "    negative_count = len(np.where(labels == 0)[0])\n",
    "    positive_count = len(np.where(labels == 1)[0])\n",
    "    total_count = negative_count + positive_count\n",
    "\n",
    "    # Build class weight dictionary.\n",
    "    return {\n",
    "        0: (1 / negative_count) * (total_count / 2),\n",
    "        1: (1 / positive_count) * (total_count / 2),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "def train(train_data, train_labels, val_data, val_labels, model):\n",
    "\n",
    "    # Take the file name from the wrapper.\n",
    "    file_path = \"../src/grain_classification/training/mil/mil_model.h5\"\n",
    "\n",
    "    # Initialize model checkpoint callback.\n",
    "    model_checkpoint = keras.callbacks.ModelCheckpoint(\n",
    "        file_path,\n",
    "        monitor=\"val_loss\",\n",
    "        verbose=0,\n",
    "        mode=\"min\",\n",
    "        save_best_only=True,\n",
    "        save_weights_only=True,\n",
    "    )\n",
    "\n",
    "    early_stopping = keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=100, mode=\"min\", restore_best_weights=True)\n",
    "    # lr_schedule = keras.optimizers.schedules.ExponentialDecay(initial_learning_rate=1e-3, decay_steps=10000, decay_rate=0.9)\n",
    "    model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.001), loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "    model.fit(\n",
    "        train_data,\n",
    "        train_labels,\n",
    "        validation_data=(val_data, val_labels),\n",
    "        epochs=150,\n",
    "        class_weight=compute_class_weights(train_labels),\n",
    "        batch_size=1,\n",
    "        callbacks=[model_checkpoint, early_stopping],\n",
    "        verbose=1,\n",
    "    )\n",
    "\n",
    "    # model.load_weights(file_path)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "model = create_model((30, 16, 16, 12))\n",
    "#print(model.summary())\n",
    "\n",
    "trained_model = train(train_data, train_labels, val_data, val_labels, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "acc=trained_model.history.history['accuracy']\n",
    "val_acc=trained_model.history.history['val_accuracy']\n",
    "loss=trained_model.history.history['loss']\n",
    "val_loss=trained_model.history.history['val_loss']\n",
    "\n",
    "plt.plot(loss, label='Train_loss')\n",
    "plt.plot(val_loss, label='Val_loss')\n",
    "plt.plot(val_acc, label='Val_acc')\n",
    "plt.plot(acc, label='Train_acc')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "model.evaluate(val_data, val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from src.utils import to_rgb\n",
    "\n",
    "def get_labels_and_bags(data, labels, bag_class, predictions=None):\n",
    "    if bag_class == \"positive\":\n",
    "        if predictions is not None:\n",
    "            print(1)\n",
    "            labels = np.where(predictions.argmax(1) == 1)[0]\n",
    "            bags = np.array(data)[:, labels[0:PLOT_SIZE]]\n",
    "            return labels, bags\n",
    "        else:\n",
    "            print(2)\n",
    "            labels = np.where(labels == 1)[0]\n",
    "            bags = np.array(data)[:, labels[0:PLOT_SIZE]]\n",
    "            return labels, bags\n",
    "    elif bag_class == \"negative\":\n",
    "        if predictions is not None:\n",
    "            print(3)\n",
    "            labels = np.where(predictions.argmax(1) == 0)[0]\n",
    "            bags = np.array(data)[:, labels[0:PLOT_SIZE]]\n",
    "            return labels, bags\n",
    "        else:\n",
    "            print(4)\n",
    "            labels = np.where(labels == 0)[0]\n",
    "            bags = np.array(data)[:, labels[0:PLOT_SIZE]]\n",
    "            return labels, bags\n",
    "\n",
    "def plot(data, labels, bag_class, predictions=None, attention_weights=None):\n",
    "\n",
    "    labels, bags = get_labels_and_bags(data, labels, bag_class, predictions=predictions)\n",
    "    labels = np.array(labels).reshape(-1)\n",
    "\n",
    "\n",
    "\n",
    "    print(f\"The bag class label is {bag_class}\")\n",
    "    for i in range(PLOT_SIZE):\n",
    "        figure = plt.figure(figsize=(8, 8))\n",
    "        print(f\"Bag number: {labels[i]}\")\n",
    "        for j in range(BAG_SIZE):\n",
    "            image = bags[j][i]\n",
    "            figure.add_subplot(1, BAG_SIZE, j + 1)\n",
    "            plt.grid(False)\n",
    "            if attention_weights is not None:\n",
    "                plt.title(np.around(attention_weights[labels[i]][j], 2))\n",
    "\n",
    "            plt.imshow(to_rgb(image[15]))\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "# Plot some of validation data bags per class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def predict(data, labels, trained_models):\n",
    "\n",
    "    # Collect info per model.\n",
    "    models_predictions = []\n",
    "    models_attention_weights = []\n",
    "    models_losses = []\n",
    "    models_accuracies = []\n",
    "\n",
    "    for model in trained_models:\n",
    "\n",
    "        # Predict output classes on data.\n",
    "        predictions = model.predict(data)\n",
    "        models_predictions.append(predictions)\n",
    "\n",
    "        # Create intermediate model to get MIL attention layer weights.\n",
    "        intermediate_model = keras.Model(model.input, model.get_layer(\"alpha\").output)\n",
    "\n",
    "        # Predict MIL attention layer weights.\n",
    "        intermediate_predictions = intermediate_model.predict(data)\n",
    "\n",
    "        attention_weights = np.squeeze(np.swapaxes(intermediate_predictions, 1, 0))\n",
    "        models_attention_weights.append(attention_weights)\n",
    "\n",
    "        loss, accuracy = model.evaluate(data, labels, verbose=0)\n",
    "        models_losses.append(loss)\n",
    "        models_accuracies.append(accuracy)\n",
    "\n",
    "    print(\n",
    "        f\"The average loss and accuracy are {np.sum(models_losses, axis=0):.2f}\"\n",
    "        f\" and {100 * np.sum(models_accuracies, axis=0):.2f} % resp.\"\n",
    "    )\n",
    "\n",
    "    return (\n",
    "        np.sum(models_predictions, axis=0),\n",
    "        np.sum(models_attention_weights, axis=0),\n",
    "    )\n",
    "\n",
    "\n",
    "# Evaluate and predict classes and attention scores on validation data.\n",
    "class_predictions, attention_params = predict(val_data, val_labels, [model])\n",
    "\n",
    "# Plot some results from our validation data.\n",
    "plot(\n",
    "    val_data,\n",
    "    val_labels,\n",
    "    \"positive\",\n",
    "    predictions=class_predictions,\n",
    "    attention_weights=attention_params,\n",
    ")\n",
    "plot(\n",
    "    val_data,\n",
    "    val_labels,\n",
    "    \"negative\",\n",
    "    predictions=class_predictions,\n",
    "    attention_weights=attention_params,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}