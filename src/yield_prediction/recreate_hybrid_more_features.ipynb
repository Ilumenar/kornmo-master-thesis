{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "from kornmo import KornmoDataset\n",
    "from geodata import get_farmer_elevation\n",
    "import kornmo_utils as ku\n",
    "from frostdataset import FrostDataset\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import tensorflow as tf\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def filter_by_years(years, data):\n",
    "    return data[data['year'].isin(years)]\n",
    "\n",
    "def get_interpolated_data(years, weather_feature):\n",
    "    data = pd.DataFrame()\n",
    "\n",
    "    print(f\"Loading {weather_feature} data...\")\n",
    "    for year in years:\n",
    "        tmp_df = pd.read_csv(f'../../kornmo-data-files/raw-data/weather-data/nn_interpolated/{weather_feature}/{weather_feature}_interpolated_{year}-03-01_to_{year}-10-01.csv')\n",
    "        tmp_df.insert(0, 'year', year)\n",
    "        data = pd.concat([data, tmp_df])\n",
    "\n",
    "    # Drop columns containing 'Unnamed'\n",
    "    data.drop(columns=[col for col in data.columns if 'Unnamed' in col], inplace=True)\n",
    "\n",
    "    return_data = ku.normalize(data.filter(regex='day_.*'))\n",
    "    columns_to_add = ['orgnr', 'year', 'longitude', 'latitude', 'elevation']\n",
    "    for i, col in enumerate(columns_to_add):\n",
    "        return_data.insert(i, col, data[col])\n",
    "\n",
    "    print(f\"Number of loaded entries: {return_data.shape[0]}\")\n",
    "    return return_data\n",
    "\n",
    "def get_proximity_data(years, weather_feature):\n",
    "    data = pd.DataFrame()\n",
    "    print(f\"Loading {weather_feature} data...\")\n",
    "    for year in years:\n",
    "        tmp_df = pd.read_csv(f'../../kornmo-data-files/raw-data/weather-data/by_proximity/{weather_feature}/{weather_feature}_by_proximity_{year}-03-01_to_{year}-10-01.csv')\n",
    "        tmp_df.drop(columns=['ws_id'], inplace=True)\n",
    "        tmp_df.insert(0, 'year', year)\n",
    "        data = pd.concat([data, tmp_df])\n",
    "\n",
    "    return_data = ku.normalize(data.filter(regex='day_.*'))\n",
    "    columns_to_add = ['orgnr', 'year']\n",
    "    for i, col in enumerate(columns_to_add):\n",
    "        return_data.insert(i, col, data[col])\n",
    "\n",
    "\n",
    "    print(f\"Number of loaded entries: {return_data.shape[0]}\")\n",
    "    return return_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "years = [2017, 2018, 2019, 2020]\n",
    "frost = FrostDataset()\n",
    "kornmo = KornmoDataset()\n",
    "deliveries = kornmo.get_deliveries().pipe(ku.split_farmers_on_type)\n",
    "\n",
    "elevation_data = get_farmer_elevation()\n",
    "deliveries = deliveries.merge(elevation_data, on=['orgnr'], how='left').fillna(0)\n",
    "\n",
    "deliveries[\"yield\"] = ku.normalize(deliveries[\"levert\"]/deliveries[\"areal\"], 0, 1000)\n",
    "deliveries[\"areal\"] = ku.normalize(deliveries[\"areal\"])\n",
    "deliveries['fulldyrket'] = ku.normalize(deliveries['fulldyrket'])\n",
    "deliveries['overflatedyrket'] = ku.normalize(deliveries['overflatedyrket'])\n",
    "deliveries['tilskudd_dyr'] = ku.normalize(deliveries['tilskudd_dyr'])\n",
    "deliveries['lat'] = ku.normalize(deliveries['lat'])\n",
    "deliveries['elevation'] = ku.normalize(deliveries['elevation'])\n",
    "\n",
    "deliveries[\"key\"] = deliveries.orgnr.astype(str) + \"/\" + deliveries.year.astype(str)\n",
    "deliveries = deliveries.set_index(\"key\")\n",
    "deliveries = filter_by_years(years, deliveries)\n",
    "\n",
    "deliveries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "historical = ku.get_historical_production(kornmo, deliveries.year.unique(), 4)\n",
    "historical = deliveries.merge(historical, how='left').fillna(0)\n",
    "historical[\"key\"] = historical.orgnr.astype(str) + \"/\" + historical.year.astype(str)\n",
    "historical = historical.drop(columns=deliveries.columns)\n",
    "historical = historical.drop_duplicates(subset='key')\n",
    "historical = historical.set_index(\"key\")\n",
    "historical = filter_by_years(years, historical)\n",
    "\n",
    "historical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sunlight_data = get_interpolated_data(years, 'sunlight')\n",
    "daydegree5_data = get_interpolated_data(years, 'daydegree5').drop(columns=['longitude', 'latitude', 'elevation'])\n",
    "ground_data = get_proximity_data(years, 'ground')\n",
    "temp_and_precip_data = frost.get_as_aggregated(1).dropna().astype(float)\n",
    "weather_data = temp_and_precip_data.merge(sunlight_data, how='left', on=['orgnr', 'year'])\n",
    "weather_data = weather_data.merge(daydegree5_data, how='left', on=['orgnr', 'year'])\n",
    "weather_data = weather_data.merge(ground_data, how='left', on=['orgnr', 'year'])\n",
    "\n",
    "print(f\"Merged {temp_and_precip_data.shape[1]} features of temp and precip data, {sunlight_data.shape[1]} features of sunlight data, {daydegree5_data.shape[1]} features of daydegree data, {ground_data.shape[1]} features of ground data to a total of {weather_data.shape[1]} features\")\n",
    "\n",
    "#weather_data = frost.get_as_aggregated(1).dropna().astype(float)\n",
    "\n",
    "weather_data[\"key\"] = weather_data.orgnr.astype(int).astype(str) + \"/\" + weather_data.year.astype(int).astype(str)\n",
    "weather_data.drop(columns=[\"year\", \"orgnr\"], inplace=True)\n",
    "weather_data = weather_data.drop_duplicates(subset=[\"key\"])\n",
    "weather_data = weather_data.set_index(\"key\")\n",
    "weather_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Combine dataset\n",
    "\n",
    "# sat_img_path = 'C:/'\n",
    "# from sentinel.storage import SentinelDataset\n",
    "# print(\"Reading sentinel_100x100_0.h5\")\n",
    "# ds0 = SentinelDataset(f\"{sat_img_path}/sentinel_100x100_0.h5\")\n",
    "# print(\"Reading sentinel_100x100_1.h5\")\n",
    "# ds1 = SentinelDataset(f\"{sat_img_path}/sentinel_100x100_1.h5\")\n",
    "# print(\"Combining both\")\n",
    "# SentinelDataset.combine_datasets([ds0, ds1], \"E:/combined_compressed.h5\", compression=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sentinel.storage import SentinelDataset\n",
    "sat_img_path = 'E:/MasterThesisData/Satellite_Images'\n",
    "#sat_img_path = 'C:/'\n",
    "sd = SentinelDataset(f\"{sat_img_path}/combined_uncompressed.h5\")\n",
    "train, val = sd.to_iterator().split(rand_seed='abc')\n",
    "\n",
    "def add_historical(orgnr, year, data):\n",
    "    if f\"{orgnr}/{year}\" in historical.index.values:\n",
    "        h_data = historical.loc[f\"{orgnr}/{year}\"]\n",
    "        return {'historical': h_data.values }\n",
    "    else:\n",
    "        return []\n",
    "def add_weather(orgnr, year, data):\n",
    "    if f\"{orgnr}/{year}\" not in weather_data.index:\n",
    "        return []\n",
    "    wd = weather_data.loc[f\"{orgnr}/{year}\"]\n",
    "\n",
    "\n",
    "    return { 'weather': wd.values }\n",
    "\n",
    "def add_grain_types(orgnr, year, data):\n",
    "    samples = deliveries.loc[[f\"{orgnr}/{year}\"]]\n",
    "\n",
    "    all_grains = []\n",
    "    for _, row in samples.iterrows():\n",
    "        sample = {}\n",
    "        if row.bygg: sample[\"type\"] = (1,0,0,0)\n",
    "        elif row.havre: sample[\"type\"] = (0,1,0,0)\n",
    "        elif row.rug_og_rughvete: sample[\"type\"] = (0,0,1,0)\n",
    "        elif row.hvete: sample[\"type\"] = (0,0,0,1)\n",
    "\n",
    "        sample[\"areal\"] = row[\"areal\"]\n",
    "        sample[\"lat\"] = row[\"lat\"]\n",
    "        sample[\"elevation\"] = row[\"elevation\"]\n",
    "        sample[\"yield\"] = row[\"yield\"]\n",
    "        sample['fulldyrket'] = row['fulldyrket']\n",
    "        sample['overflatedyrket'] = row['overflatedyrket']\n",
    "        sample['tilskudd_dyr'] = row['tilskudd_dyr']\n",
    "        all_grains.append(sample)\n",
    "\n",
    "    return all_grains\n",
    "\n",
    "train = train.with_data(add_historical, True)\\\n",
    "             .with_data(add_weather, True)\\\n",
    "             .with_data(add_grain_types, True)\n",
    "\n",
    "val = val.with_data(add_historical, True)\\\n",
    "         .with_data(add_weather, True)\\\n",
    "         .with_data(add_grain_types, True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from mask.mask_dataset import MaskDataset\n",
    "from mask.utils import add_mask_as_channel, apply_mask_to_image_series\n",
    "\n",
    "mask_dataset_path = \"data/masks/nibio_disposed_properties_masks.h5\"\n",
    "mask_dataset = MaskDataset(mask_dataset_path)\n",
    "#print(mask_dataset.labels)\n",
    "\n",
    "mask_iterator = mask_dataset.get_iterator()\n",
    "mask_dict = {}\n",
    "for orgnr, year, mask in mask_iterator:\n",
    "    mask_dict[f'{orgnr}/{year}'] = mask\n",
    "\n",
    "def apply_mask(orgnr, year, imgs):\n",
    "    mask = mask_dict[f'{orgnr}/{year}']\n",
    "    return apply_mask_to_image_series(mask, imgs)\n",
    "\n",
    "train = train.filter(lambda orgnr, year, _,__: f\"{orgnr}/{year}\" in mask_dict)\n",
    "val = val.filter(lambda orgnr, year, _,__: f\"{orgnr}/{year}\" in mask_dict)\n",
    "\n",
    "print(f\"train samples: {len(train)}\")\n",
    "print(f\"val samples: {len(val)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow_addons as tfa\n",
    "import numpy as np\n",
    "from tensorflow.data.experimental import assert_cardinality\n",
    "from sentinel.transform import salt_n_pepper, rotate180, rotate90\n",
    "\n",
    "stride = 10\n",
    "def top_left(imgs):\n",
    "    return imgs[...,:-stride, :-stride,:]\n",
    "def top_right(imgs):\n",
    "    return imgs[...,:-stride, stride:,:]\n",
    "def bot_left(imgs):\n",
    "    return imgs[...,stride:, :-stride,:]\n",
    "def bot_right(imgs):\n",
    "    return imgs[...,stride:, stride:,:]\n",
    "def center(imgs):\n",
    "    s = stride//2\n",
    "    return imgs[...,s:-s, s:-s,:]\n",
    "\n",
    "def rotate_random(imgs):\n",
    "    angle = np.random.rand(30) * 6.28\n",
    "    return tfa.image.rotate(imgs, angle)\n",
    "\n",
    "augmented_dataset = train\\\n",
    "    .transform(apply_mask)\\\n",
    "    .transform(salt_n_pepper())\\\n",
    "    .augment([center, top_left, top_right, bot_left, bot_right], keep_original=False)\\\n",
    "    .transform(rotate_random)\n",
    "\n",
    "def apply_output(orgnr, year, img_source, data):\n",
    "    features = data[\"areal\"], *data[\"type\"]\n",
    "    output = data[\"yield\"]\n",
    "    weather = data[\"weather\"][1:]\n",
    "    return {\"cnn_input\": img_source[0:30], \"feature_input\": features, \"weather_input\": weather}, output\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_generator(\n",
    "    augmented_dataset.apply(apply_output).shuffled(),\n",
    "    output_types=({\"cnn_input\": tf.dtypes.float64, \"feature_input\": tf.dtypes.float64, \"weather_input\": tf.dtypes.float64}, tf.dtypes.float64),\n",
    ").apply(assert_cardinality(len(augmented_dataset)))\n",
    "\n",
    "val_dataset = tf.data.Dataset.from_generator(\n",
    "    val.transform(apply_mask).transform(center).apply(apply_output),\n",
    "    output_types=({\"cnn_input\": tf.dtypes.float64, \"feature_input\": tf.dtypes.float64, \"weather_input\": tf.dtypes.float64}, tf.dtypes.float64),\n",
    ").apply(assert_cardinality(len(val)))\n",
    "\n",
    "print(f\"Augmented samples: {len(train_dataset)}\")\n",
    "print(f\"Validation samples: {len(val_dataset)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers, models, optimizers\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow import keras\n",
    "\n",
    "\n",
    "\n",
    "def CNN(input_dim, output_dim):\n",
    "    input_layer = layers.Input(shape=input_dim)\n",
    "    y = layers.Conv2D(16, (3, 3), activation=tf.nn.relu, padding='same')(input_layer)\n",
    "    y = layers.MaxPool2D((2, 2))(y)\n",
    "    y = layers.Conv2D(32, (3, 3), activation=tf.nn.relu, padding='same')(y)\n",
    "    y = layers.MaxPool2D((2, 2))(y)\n",
    "    y = layers.Conv2D(64, (3, 3), activation=tf.nn.relu, padding='same')(y)\n",
    "    y = layers.MaxPool2D((2, 2))(y)\n",
    "    y = layers.Flatten()(y)\n",
    "    y = layers.Dense(output_dim, activation=tf.nn.relu)(y)\n",
    "\n",
    "    return models.Model(inputs=[input_layer], outputs=[y], name=\"SingleImageCNN\")\n",
    "\n",
    "file_path = \"./training/yield_hybrid/hybrid_yield_model.h5\"\n",
    "model_checkpoint = keras.callbacks.ModelCheckpoint(\n",
    "    file_path,\n",
    "    monitor=\"val_loss\",\n",
    "    verbose=0,\n",
    "    mode=\"min\",\n",
    "    save_best_only=True,\n",
    "    save_weights_only=True,\n",
    ")\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "callbacks = [callback, model_checkpoint]\n",
    "\n",
    "restart = False\n",
    "if restart:\n",
    "\n",
    "    scnn = CNN((90, 90, 12), 64)\n",
    "    #scnn.summary(line_length=130)\n",
    "    input_weather = layers.Input(shape=1501, name=\"weather_input\") #shape = 856 / 1501\n",
    "    t_wm = layers.Reshape((19, 79))(input_weather) # (4, 214) / (19, 79)\n",
    "    t_wm = layers.Permute((2, 1))(t_wm)\n",
    "    t_wm = layers.Conv1D(64, 50, activation=tf.nn.relu)(t_wm) # (64, 7, 7) / (64, 50)\n",
    "\n",
    "    input_cnn = layers.Input(shape=(30, 90, 90, 12), name=\"cnn_input\")\n",
    "\n",
    "    feature_input = layers.Input(shape=(5,), name=\"feature_input\")\n",
    "    feature_repeated = layers.RepeatVector(30)(feature_input)\n",
    "\n",
    "    cnn = layers.TimeDistributed(scnn)(input_cnn)\n",
    "    cnn = layers.Concatenate(axis=2)([cnn, feature_repeated, t_wm])\n",
    "    cnn = layers.GRU(128, return_sequences=False)(cnn)\n",
    "    cnn = layers.Flatten()(cnn)\n",
    "    cnn = layers.Dense(128, activation=tf.nn.relu)(cnn)\n",
    "    cnn = layers.Dense(1)(cnn)\n",
    "\n",
    "    cnn = models.Model(inputs=[input_weather, input_cnn, feature_input], outputs=cnn, name=\"CNN\")\n",
    "    #cnn.summary(line_length=130)\n",
    "\n",
    "    cnn.compile(optimizer=optimizers.Adam(), loss='mean_absolute_error')\n",
    "\n",
    "    cnn_history = cnn.fit(\n",
    "        train_dataset.take(10000).batch(32).prefetch(2),\n",
    "        validation_data=val_dataset.batch(32).prefetch(2),\n",
    "        epochs=10,\n",
    "        verbose=1,\n",
    "        callbacks=callbacks\n",
    "    )\n",
    "\n",
    "else:\n",
    "    cnn = load_model('./training/yield_hybrid/epoch_10.hdf5')\n",
    "    # update the learning rate\n",
    "    cnn_history = cnn.fit(\n",
    "        train_dataset.take(10000).batch(32).prefetch(2),\n",
    "        validation_data=val_dataset.batch(32).prefetch(2),\n",
    "        epochs=10,\n",
    "        verbose=1,\n",
    "        callbacks=callbacks\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "cnn = load_model('./training/yield_hybrid/epoch_10.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "cnn.evaluate(val_dataset.batch(32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "sns.set_style('whitegrid')\n",
    "sns.set_context(\"paper\")\n",
    "\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.plot(history['loss'].tolist(), label=\"loss\")\n",
    "plt.plot(history['val_loss'].tolist(), label=\"val_loss\")\n",
    "plt.legend()\n",
    "plt.title(\"Mean absolute error loss\")\n",
    "plt.savefig('logs/hybrid_more_features.svg', dpi=600)\n",
    "plt.grid()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}