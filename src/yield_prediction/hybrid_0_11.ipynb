{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "from kornmo import KornmoDataset\n",
    "from geodata import get_farmer_elevation\n",
    "import kornmo_utils as ku\n",
    "from frostdataset import FrostDataset\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "TIMESTEPS = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def filter_by_years(years, data):\n",
    "    return data[data['year'].isin(years)]\n",
    "\n",
    "\n",
    "def get_interpolated_data(years, weather_feature):\n",
    "    data = pd.DataFrame()\n",
    "\n",
    "    print(f\"Loading {weather_feature} data...\")\n",
    "    for year in years:\n",
    "        tmp_df = pd.read_csv(f'../../kornmo-data-files/raw-data/weather-data/nn_interpolated/{weather_feature}/{weather_feature}_interpolated_{year}-03-01_to_{year}-10-01.csv')\n",
    "        tmp_df.insert(0, 'year', year)\n",
    "        data = pd.concat([data, tmp_df])\n",
    "\n",
    "    # Drop columns containing 'Unnamed'\n",
    "    data.drop(columns=[col for col in data.columns if 'Unnamed' in col], inplace=True)\n",
    "\n",
    "    return_data = ku.normalize(data.filter(regex='day_.*'))\n",
    "    return_data = return_data.rename(columns=lambda x: f\"{weather_feature + x[4:]}\")\n",
    "    columns_to_add = ['orgnr', 'year', 'longitude', 'latitude', 'elevation']\n",
    "    for i, col in enumerate(columns_to_add):\n",
    "        return_data.insert(i, col, data[col])\n",
    "\n",
    "    print(f\"Number of loaded entries: {return_data.shape[0]}\")\n",
    "    return return_data\n",
    "\n",
    "def get_proximity_data(years, weather_feature):\n",
    "    data = pd.DataFrame()\n",
    "    print(f\"Loading {weather_feature} data...\")\n",
    "    for year in years:\n",
    "        tmp_df = pd.read_csv(f'../../kornmo-data-files/raw-data/weather-data/by_proximity/{weather_feature}/{weather_feature}_by_proximity_{year}-03-01_to_{year}-10-01.csv')\n",
    "        tmp_df.drop(columns=['ws_id'], inplace=True)\n",
    "        tmp_df.insert(0, 'year', year)\n",
    "        data = pd.concat([data, tmp_df])\n",
    "\n",
    "    return_data = ku.normalize(data.filter(regex='day_.*'))\n",
    "    return_data = return_data.rename(columns=lambda x: f\"{weather_feature + x[4:]}\")\n",
    "\n",
    "    columns_to_add = ['orgnr', 'year']\n",
    "    for i, col in enumerate(columns_to_add):\n",
    "        return_data.insert(i, col, data[col])\n",
    "\n",
    "\n",
    "    print(f\"Number of loaded entries: {return_data.shape[0]}\")\n",
    "    return return_data\n",
    "\n",
    "def get_soilquality_data():\n",
    "    data = pd.read_csv(f'../../kornmo-data-files/raw-data/farm-information/farmers-with-coordinates-and-soil_quality.csv')\n",
    "    data.drop(columns=['Unnamed: 0', 'latitude', 'longitude', 'elevation'], inplace=True)\n",
    "    return_data = ku.normalize(data.drop(columns=['orgnr']))\n",
    "    return_data.insert(0, 'orgnr', data['orgnr'])\n",
    "    return return_data\n",
    "\n",
    "\n",
    "def get_area_and_croptype():\n",
    "    data = pd.read_csv('../../kornmo-data-files/raw-data/crop-classification-data/week_1_11/field_areas.csv')\n",
    "    data.drop(columns=['Unnamed: 0'], inplace=True)\n",
    "    data.drop(data[data['area'] < 1500].index, inplace = True)\n",
    "    data[\"area\"] = ku.normalize(data[\"area\"])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load deliveries, weather, and historical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "years = [2017, 2018, 2019]\n",
    "\n",
    "frost = FrostDataset()\n",
    "kornmo = KornmoDataset()\n",
    "deliveries = kornmo.get_deliveries().pipe(ku.split_farmers_on_type)\n",
    "\n",
    "elevation_data = get_farmer_elevation()\n",
    "deliveries = deliveries.merge(elevation_data, on=['orgnr'], how='left').fillna(0)\n",
    "\n",
    "deliveries[\"yield\"] = ku.normalize(deliveries[\"levert\"]/deliveries[\"areal\"], 0, 1000)\n",
    "deliveries[\"areal\"] = ku.normalize(deliveries[\"areal\"])\n",
    "deliveries['fulldyrket'] = ku.normalize(deliveries['fulldyrket'])\n",
    "deliveries['overflatedyrket'] = ku.normalize(deliveries['overflatedyrket'])\n",
    "deliveries['tilskudd_dyr'] = ku.normalize(deliveries['tilskudd_dyr'])\n",
    "deliveries['lat'] = ku.normalize(deliveries['lat'])\n",
    "deliveries['elevation'] = ku.normalize(deliveries['elevation'])\n",
    "\n",
    "deliveries[\"key\"] = deliveries.orgnr.astype(str) + \"/\" + deliveries.year.astype(str)\n",
    "deliveries = deliveries.set_index(\"key\")\n",
    "deliveries = filter_by_years(years, deliveries)\n",
    "\n",
    "deliveries\n",
    "# test = deliveries.loc[\"869093262/2017\"]\n",
    "# test.loc[test['bygg'] == 2.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "historical = ku.get_historical_production(kornmo, [2017, 2018, 2019], 4)\n",
    "historical = deliveries.merge(historical, how='left').fillna(0)\n",
    "historical[\"key\"] = historical.orgnr.astype(str) + \"/\" + historical.year.astype(str)\n",
    "historical = historical.drop(columns=deliveries.columns)\n",
    "historical = historical.drop_duplicates(subset='key')\n",
    "historical = historical.set_index(\"key\")\n",
    "historical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sunlight_data = get_interpolated_data(years, 'sunlight')\n",
    "daydegree5_data = get_interpolated_data(years, 'daydegree5').drop(columns=['longitude', 'latitude', 'elevation'])\n",
    "ground_data = get_proximity_data(years, 'ground')\n",
    "\n",
    "temp_and_precip_data = frost.get_as_aggregated(1).dropna().astype(float)\n",
    "weather_data = temp_and_precip_data.merge(sunlight_data, how='left', on=['orgnr', 'year'])\n",
    "weather_data = weather_data.merge(daydegree5_data, how='left', on=['orgnr', 'year'])\n",
    "weather_data = weather_data.merge(ground_data, how='left', on=['orgnr', 'year'])\n",
    "\n",
    "print(f\"Merged {temp_and_precip_data.shape[1]} features of temp and precip data, {sunlight_data.shape[1]} features of sunlight data, {daydegree5_data.shape[1]} features of daydegree data, {ground_data.shape[1]} features of ground data to a total of {weather_data.shape[1]} features\")\n",
    "\n",
    "#weather_data = frost.get_as_aggregated(1).dropna().astype(float)\n",
    "\n",
    "weather_data[\"key\"] = weather_data.orgnr.astype(int).astype(str) + \"/\" + weather_data.year.astype(int).astype(str)\n",
    "weather_data = filter_by_years(years, weather_data)\n",
    "weather_data.drop(columns=[\"year\", \"orgnr\"], inplace=True)\n",
    "weather_data = weather_data.drop_duplicates(subset=[\"key\"])\n",
    "weather_data = weather_data.set_index(\"key\")\n",
    "\n",
    "weather_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "soilquality_data = get_soilquality_data()\n",
    "soilquality_data[\"key\"] = soilquality_data.orgnr.astype(int).astype(str)\n",
    "soilquality_data.drop(columns=[\"orgnr\"], inplace=True)\n",
    "soilquality_data = soilquality_data.drop_duplicates(subset=[\"key\"])\n",
    "soilquality_data = soilquality_data.set_index(\"key\")\n",
    "soilquality_data.dropna(inplace=True)\n",
    "soilquality_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "area_data = get_area_and_croptype()\n",
    "area_data[\"key\"] = area_data.orgnr.astype(int).astype(str) + \"/\" + area_data.year.astype(int).astype(str)\n",
    "print(len(set(area_data['orgnr'])))\n",
    "area_data.drop(columns=[\"year\", \"orgnr\"], inplace=True)\n",
    "#area_data = area_data.drop_duplicates(subset=[\"key\"])\n",
    "area_data = area_data.set_index(\"key\")\n",
    "area_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "weather_data.loc[\"997690877/2019\"].filter(regex='total_rain(([1-7]?[0-9])|(8[0-3]))$')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split training and validation data, and add data to the image iterators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "from sentinel.storage import SentinelDataset\n",
    "sat_img_path = 'E:/MasterThesisData/Satellite_Images'\n",
    "sd = SentinelDataset(\"E:/MasterThesisData/Satellite_Images/combined_uncompressed.h5\")\n",
    "train, val = sd.to_iterator().split(rand_seed='abc')\n",
    "\n",
    "\n",
    "def add_historical(orgnr, year, data):\n",
    "    if f\"{orgnr}/{year}\" in historical.index.values:\n",
    "        h_data = historical.loc[f\"{orgnr}/{year}\"]\n",
    "        return {'historical': h_data.values }\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "\n",
    "def add_soilquality(orgnr, year, data):\n",
    "    if str(orgnr) in soilquality_data.index.values:\n",
    "        soil_data = soilquality_data.loc[orgnr]\n",
    "        return {'soil_data': soil_data.values}\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "\n",
    "def add_weather(orgnr, year, data):\n",
    "    if f\"{orgnr}/{year}\" not in weather_data.index:\n",
    "        return []\n",
    "\n",
    "    wd = weather_data.loc[f\"{orgnr}/{year}\"]\n",
    "\n",
    "    min_temps = wd.filter(regex='min_temp(([1-7]?[0-9])|(8[0-3]))$').values\n",
    "    mean_temps = wd.filter(regex='mean_temp(([1-7]?[0-9])|(8[0-3]))$').values\n",
    "    max_temps = wd.filter(regex='max_temp(([1-7]?[0-9])|(8[0-3]))$').values\n",
    "    total_rain = wd.filter(regex='total_rain(([1-7]?[0-9])|(8[0-3]))$').values\n",
    "\n",
    "    sunlight = wd.filter(regex='sunlight(([1-7]?[0-9])|(8[0-3]))$').values\n",
    "    daydegree5 = wd.filter(regex='daydegree5(([1-7]?[0-9])|(8[0-3]))$').values\n",
    "    ground = wd.filter(regex='ground(([1-7]?[0-9])|(8[0-3]))$').values\n",
    "\n",
    "    assert len(min_temps) == len(mean_temps) == len(max_temps) == len(total_rain) == len(sunlight) == len(daydegree5) == len(ground) == TIMESTEPS*7\n",
    "    wd = np.concatenate((min_temps, mean_temps, max_temps, total_rain, sunlight, daydegree5, ground), axis=0)\n",
    "\n",
    "    return { 'weather': wd }\n",
    "\n",
    "def add_grain_types(orgnr, year, data):\n",
    "    samples = deliveries.loc[[f\"{orgnr}/{year}\"]]\n",
    "    if f\"{orgnr}/{year}\" in area_data.index.values:\n",
    "        farm_area = area_data.loc[[f\"{orgnr}/{year}\"]]\n",
    "        all_data = []\n",
    "\n",
    "        for i, row in farm_area.iterrows():\n",
    "            sample = {}\n",
    "            if row['crop_type'] == 'bygg': sample[\"type\"] = (1,0,0)\n",
    "            elif row['crop_type'] == 'havre': sample[\"type\"] = (0,1,0)\n",
    "            elif row['crop_type'] == 'hvete': sample[\"type\"] = (0,0,1)\n",
    "\n",
    "            sample['area'] = row['area']\n",
    "\n",
    "            if isinstance(samples, pd.DataFrame):\n",
    "\n",
    "                del_sample = samples.loc[samples[row['crop_type']] == 1.0]\n",
    "                if len(del_sample.index) > 0:\n",
    "                    sample[\"lat\"] = del_sample[\"lat\"].values[0]\n",
    "                    sample[\"elevation\"] = del_sample[\"elevation\"].values[0]\n",
    "                    sample[\"yield\"] = del_sample[\"yield\"].values[0]\n",
    "                    all_data.append(sample)\n",
    "\n",
    "            else:\n",
    "                del_sample = samples\n",
    "                sample[\"lat\"] = del_sample[\"lat\"]\n",
    "                sample[\"elevation\"] = del_sample[\"elevation\"]\n",
    "                sample[\"yield\"] = del_sample[\"yield\"]\n",
    "                all_data.append(sample)\n",
    "\n",
    "\n",
    "        return all_data\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "\n",
    "\n",
    "train = train.with_data(add_historical, True)\\\n",
    "             .with_data(add_weather, True)\\\n",
    "             .with_data(add_grain_types, True)\\\n",
    "             .with_data(add_soilquality, True)\n",
    "\n",
    "val = val.with_data(add_historical, True)\\\n",
    "         .with_data(add_weather, True)\\\n",
    "         .with_data(add_grain_types, True)\\\n",
    "         .with_data(add_soilquality, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(len(train))\n",
    "for test_data in train:\n",
    "    print(test_data)\n",
    "    break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from mask.mask_dataset_classification import MaskDatasetClassification\n",
    "from mask.utils import add_mask_as_channel, apply_mask_to_image_series\n",
    "\n",
    "mask_dataset_path = \"../../kornmo-data-files/raw-data/crop-classification-data/week_1_11/week_1_11_masks.h5\"\n",
    "\n",
    "\n",
    "mask_dataset = MaskDatasetClassification(mask_dataset_path)\n",
    "mask_iterator = mask_dataset.get_iterator()\n",
    "mask_dict = {}\n",
    "\n",
    "classes = ['bygg', 'havre', 'hvete']\n",
    "\n",
    "for orgnr, year, crop_type, mask in tqdm(mask_iterator, total=mask_iterator.n):\n",
    "    mask_dict[f'{orgnr}/{year}/{crop_type}'] = mask\n",
    "\n",
    "def apply_mask(orgnr, year, imgs, data):\n",
    "    crop_type = classes[data['type'].index(1)]\n",
    "\n",
    "    mask = mask_dict[f\"{orgnr}/{year}/{crop_type}\"]\n",
    "    return apply_mask_to_image_series(mask, imgs)\n",
    "\n",
    "train = train.filter(lambda orgnr, year, _,data: f\"{orgnr}/{year}/{classes[data['type'].index(1)]}\" in mask_dict)\n",
    "val = val.filter(lambda orgnr, year, _,data: f\"{orgnr}/{year}/{classes[data['type'].index(1)]}\" in mask_dict)\n",
    "\n",
    "print(f\"train samples: {len(train)}\")\n",
    "print(f\"val samples: {len(val)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "import numpy as np\n",
    "from tensorflow.data.experimental import assert_cardinality\n",
    "from sentinel.transform import salt_n_pepper, rotate180, rotate90\n",
    "import matplotlib.pyplot as plt\n",
    "from src.utils import to_rgb\n",
    "\n",
    "timesteps = 12\n",
    "\n",
    "stride = 10\n",
    "def top_left(imgs):\n",
    "    return imgs[...,:-stride, :-stride,:]\n",
    "def top_right(imgs):\n",
    "    return imgs[...,:-stride, stride:,:]\n",
    "def bot_left(imgs):\n",
    "    return imgs[...,stride:, :-stride,:]\n",
    "def bot_right(imgs):\n",
    "    return imgs[...,stride:, stride:,:]\n",
    "def center(imgs):\n",
    "    s = stride//2\n",
    "    return imgs[...,s:-s, s:-s,:]\n",
    "\n",
    "def rotate_random(imgs):\n",
    "    #angle = np.random.rand(timesteps) * 6.28\n",
    "    angle = tf.constant(np.pi/8)\n",
    "    return tfa.image.rotate(imgs, angle)\n",
    "\n",
    "augmented_dataset = train\\\n",
    "    .transform(apply_mask)\\\n",
    "    .augment([center, top_left, top_right, bot_left, bot_right], keep_original=False)\\\n",
    "    .transform(salt_n_pepper())\\\n",
    "    .transform(rotate_random)\n",
    "\n",
    "\n",
    "def apply_output(orgnr, year, img_source, data):\n",
    "    features = data[\"area\"], *data[\"type\"], *data[\"soil_data\"]\n",
    "    output = data[\"yield\"]\n",
    "    weather = data[\"weather\"]\n",
    "    return {\"cnn_input\": img_source[0:timesteps], \"feature_input\": features, \"weather_input\": weather}, output\n",
    "\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_generator(\n",
    "    augmented_dataset.apply(apply_output).shuffled(),\n",
    "    output_types=({\"cnn_input\": tf.dtypes.float64, \"feature_input\": tf.dtypes.float64, \"weather_input\": tf.dtypes.float64}, tf.dtypes.float64),\n",
    ").apply(assert_cardinality(len(augmented_dataset)))\n",
    "\n",
    "\n",
    "val_dataset = tf.data.Dataset.from_generator(\n",
    "    val.transform(apply_mask).transform(center).apply(apply_output),\n",
    "    output_types=({\"cnn_input\": tf.dtypes.float64, \"feature_input\": tf.dtypes.float64, \"weather_input\": tf.dtypes.float64}, tf.dtypes.float64),\n",
    ").apply(assert_cardinality(len(val)))\n",
    "\n",
    "print(f\"Training samples: {len(train)}\")\n",
    "print(f\"Augmented samples: {len(train_dataset)}\")\n",
    "print(f\"Validation samples: {len(val_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers, models, optimizers\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow import keras\n",
    "\n",
    "\n",
    "def CNN(input_dim, output_dim):\n",
    "    input_layer = layers.Input(shape=input_dim)\n",
    "    y = layers.Conv2D(16, (3, 3), activation=tf.nn.relu, padding='same')(input_layer)\n",
    "    y = layers.MaxPool2D((2, 2))(y)\n",
    "    y = layers.Conv2D(32, (3, 3), activation=tf.nn.relu, padding='same')(y)\n",
    "    y = layers.MaxPool2D((2, 2))(y)\n",
    "    y = layers.Conv2D(64, (3, 3), activation=tf.nn.relu, padding='same')(y)\n",
    "    y = layers.MaxPool2D((2, 2))(y)\n",
    "    y = layers.Flatten()(y)\n",
    "    y = layers.Dense(output_dim, activation=tf.nn.relu)(y)\n",
    "\n",
    "    return models.Model(inputs=[input_layer], outputs=[y], name=\"SingleImageCNN\")\n",
    "\n",
    "scnn = CNN((90, 90, 12), 64)\n",
    "# scnn.summary(line_length=130)\n",
    "\n",
    "input_weather = layers.Input(shape=timesteps*7*7, name=\"weather_input\")\n",
    "t_wm = layers.Reshape((7, timesteps*7))(input_weather)\n",
    "t_wm = layers.Permute((2, 1))(t_wm)\n",
    "t_wm = layers.Conv1D(64, 7, 7, activation=tf.nn.relu)(t_wm)\n",
    "\n",
    "input_cnn = layers.Input(shape=(timesteps, 90, 90, 12), name=\"cnn_input\")\n",
    "\n",
    "feature_input = layers.Input(shape=(9,), name=\"feature_input\")\n",
    "feature_repeated = layers.RepeatVector(timesteps)(feature_input)\n",
    "\n",
    "cnn = layers.TimeDistributed(scnn)(input_cnn)\n",
    "cnn = layers.Concatenate(axis=2)([cnn, feature_repeated, t_wm])\n",
    "cnn = layers.GRU(128)(cnn)\n",
    "cnn = layers.Flatten()(cnn)\n",
    "cnn = layers.Dense(128, activation=tf.nn.relu)(cnn)\n",
    "cnn = layers.Dense(1)(cnn)\n",
    "\n",
    "cnn = models.Model(inputs=[input_weather, input_cnn, feature_input], outputs=cnn, name=\"CNN\")\n",
    "# cnn.summary(line_length=130)\n",
    "\n",
    "cnn.compile(optimizer=optimizers.Adam(), loss='mean_absolute_error')\n",
    "\n",
    "model_checkpoint = keras.callbacks.ModelCheckpoint(\n",
    "    './training/1_11.h5',\n",
    "    monitor=\"val_loss\",\n",
    "    verbose=0,\n",
    "    mode=\"min\",\n",
    "    save_best_only=True,\n",
    "    save_weights_only=True,\n",
    ")\n",
    "\n",
    "callbacks = [tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True), model_checkpoint]\n",
    "\n",
    "cnn_history = cnn.fit(\n",
    "    train_dataset.take(10000).batch(32).prefetch(2),\n",
    "    validation_data=val_dataset.batch(32).prefetch(2),\n",
    "    epochs=20,\n",
    "    verbose=1,\n",
    "    callbacks=callbacks\n",
    ")\n",
    "\n",
    "# model = load_model('./training/epoch_100.hdf5')\n",
    "#\n",
    "# cnn_history = model.fit(\n",
    "#         train_dataset.take(10000).batch(32).prefetch(2),\n",
    "#         validation_data=val_dataset.batch(32).prefetch(2),\n",
    "#         epochs=100,\n",
    "#         verbose=1,\n",
    "#         callbacks=callbacks\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plt.xlabel('Epochs')\n",
    "plt.ylabel(\"Loss/Accuracy\")\n",
    "plt.plot(cnn_history.history['loss'], label=\"Training Loss\")\n",
    "plt.plot(cnn_history.history['val_loss'], label=\"Validation Loss\")\n",
    "\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.savefig('early_yield_0_11.svg', dpi=600)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = cnn.predict(val_dataset.batch(32).prefetch(2))\n",
    "predictions = np.array(predictions).flatten()\n",
    "facts = val.apply(lambda orgnr, year, img, data: data[\"yield\"]).as_array()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def denormalize(df, lower: float, upper: float):\n",
    "    return df * (upper - lower) + lower\n",
    "\n",
    "absolute_error = np.abs(predictions - facts)\n",
    "\n",
    "print(f\"Denormalized MAE: {denormalize(absolute_error.mean(), 0, 1000)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  },
  "toc-showcode": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}